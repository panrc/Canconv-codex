2025-06-04 00:55:03,671 INFO Train script invoked with args: ['/home/panrc/project/CANConv-1/canconv/scripts/train.py', 'cannet', 'wv3']
2025-06-04 00:55:03,679 INFO Begin to load kmeans operator...
2025-06-04 00:55:03,695 INFO Finish loading kmeans operator.
2025-06-04 00:55:03,698 INFO Loading custom op for conv_by_cluster...
2025-06-04 00:55:07,624 INFO Seed set to 10
2025-06-04 00:55:07,624 INFO Using device: cuda:0
2025-06-04 00:55:18,519 ERROR CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 10.48 GiB is allocated by PyTorch, and 6.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/panrc/project/CANConv-1/canconv/scripts/train.py", line 58, in <module>
    main(args.model_name, cfg, args.save_mat, args.preset, args.override)
  File "/home/panrc/project/CANConv-1/canconv/scripts/train.py", line 24, in main
    trainer = module.Trainer(cfg)
              ^^^^^^^^^^^^^^^^^^^
  File "/home/panrc/project/CANConv-1/canconv/models/cannet/config.py", line 13, in __init__
    super().__init__(cfg)
  File "/home/panrc/project/CANConv-1/canconv/util/trainer.py", line 67, in __init__
    self.forward({
  File "/home/panrc/project/CANConv-1/canconv/models/cannet/config.py", line 75, in forward
    return self.model(pan_image, lms_image)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/panrc/project/CANConv-1/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/panrc/project/CANConv-1/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/panrc/project/CANConv-1/canconv/models/cannet/model.py", line 221, in forward
    x1_transformed = self.transformer1(x1_rb_out)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/panrc/project/CANConv-1/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/panrc/project/CANConv-1/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/panrc/project/CANConv-1/canconv/models/cannet/model.py", line 34, in forward
    attn_out, _ = self.attention(x_seq, x_seq, x_seq)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/panrc/project/CANConv-1/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/panrc/project/CANConv-1/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/panrc/project/CANConv-1/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/panrc/project/CANConv-1/.venv/lib/python3.12/site-packages/torch/nn/functional.py", line 6376, in multi_head_attention_forward
    attn_output_weights = dropout(attn_output_weights, p=dropout_p)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/panrc/project/CANConv-1/.venv/lib/python3.12/site-packages/torch/nn/functional.py", line 1425, in dropout
    _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 10.48 GiB is allocated by PyTorch, and 6.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
